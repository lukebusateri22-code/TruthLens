# Project Architecture

Detailed technical documentation of the deepfake detection system architecture.

## ğŸ“ System Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Deepfake Detection System                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚
â”‚  â”‚   Part 1:    â”‚         â”‚   Part 2:    â”‚                  â”‚
â”‚  â”‚ Centralized  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Federated   â”‚                  â”‚
â”‚  â”‚   Training   â”‚         â”‚   Learning   â”‚                  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚
â”‚         â”‚                         â”‚                          â”‚
â”‚         â–¼                         â–¼                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚
â”‚  â”‚      Deepfake Detection Model        â”‚                   â”‚
â”‚  â”‚  (EfficientNet / ResNet / Hybrid)    â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—ï¸ Module Structure

### 1. Models Module (`models/`)

#### `deepfake_detector.py`
Contains three model architectures:

**DeepfakeDetector (EfficientNet-based)**
```
Input (3, 224, 224)
    â†“
EfficientNet-B0 Backbone (pretrained on ImageNet)
    â†“
Feature Vector (1280 dimensions)
    â†“
Custom Classification Head:
    - Linear(1280 â†’ 512) + ReLU + Dropout(0.5)
    - Linear(512 â†’ 256) + ReLU + Dropout(0.5)
    - Linear(256 â†’ 2)
    â†“
Output Logits (2 classes: Real/Fake)
```

**HybridDeepfakeDetector**
- Adds attention mechanism
- Batch normalization layers
- Enhanced feature extraction

**ResNetDeepfakeDetector**
- ResNet-50 backbone
- Similar classification head
- Alternative architecture for comparison

#### `model_utils.py`
Utility functions:
- `save_checkpoint()`: Save model state
- `load_checkpoint()`: Load model state
- `calculate_metrics()`: Compute evaluation metrics
- `EarlyStopping`: Prevent overfitting
- `AverageMeter`: Track running statistics

### 2. Data Module (`data/`)

#### `preprocessing.py`

**VideoFrameExtractor**
```
Video File (.mp4, .avi, etc.)
    â†“
Extract N frames (uniform/random/first)
    â†“
Resize to (224, 224)
    â†“
RGB Frames (N, 224, 224, 3)
```

**Data Augmentation Pipeline**
```
Training:
- Resize(224, 224)
- HorizontalFlip(p=0.5)
- RandomRotate90(p=0.3)
- Brightness/Contrast adjustment
- Gaussian Noise/Blur
- ShiftScaleRotate
- CoarseDropout
- Normalize (ImageNet stats)
- ToTensor

Validation/Test:
- Resize(224, 224)
- Normalize
- ToTensor
```

**FaceDetector**
- Haar Cascade face detection
- Extract largest face
- Focus on facial regions

#### `data_loader.py`

**DeepfakeDataset**
```
Directory Structure:
data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ real/
â”‚   â””â”€â”€ fake/
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ real/
â”‚   â””â”€â”€ fake/
â””â”€â”€ test/
    â”œâ”€â”€ real/
    â””â”€â”€ fake/

Dataset loads images/videos and applies transforms
Returns: (image_tensor, label)
```

**Data Partitioning for FL**
```
IID Partitioning:
- Randomly shuffle all data
- Split evenly among N clients
- Each client has balanced distribution

Non-IID Partitioning:
- Use Dirichlet distribution with parameter Î±
- Lower Î± â†’ more heterogeneous
- Each client has skewed class distribution
```

### 3. Federated Learning Module (`federated/`)

#### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Federated Server                      â”‚
â”‚  - Aggregates model updates                             â”‚
â”‚  - Manages global model                                 â”‚
â”‚  - Coordinates training rounds                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â”‚ Global Model
             â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                 â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”
â”‚Client1â”‚  â”‚Client2â”‚  â”‚ClientNâ”‚
â”‚       â”‚  â”‚       â”‚  â”‚       â”‚
â”‚ Data1 â”‚  â”‚ Data2 â”‚  â”‚ DataN â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”˜
    â”‚          â”‚          â”‚
    â”‚ Updates  â”‚ Updates  â”‚ Updates
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### `client.py`

**DeepfakeClient**
```python
class DeepfakeClient:
    def fit(parameters, config):
        # 1. Receive global model parameters
        # 2. Train on local data for E epochs
        # 3. Return updated parameters
        
    def evaluate(parameters, config):
        # 1. Receive model parameters
        # 2. Evaluate on local validation data
        # 3. Return loss and metrics
```

#### `server.py`

**FedAvg Strategy**
```
For each round t = 1, 2, ..., T:
    1. Server sends global model w_t to clients
    
    2. Each client k:
       - Trains on local data D_k
       - Computes update Î”w_k
       
    3. Server aggregates:
       w_{t+1} = Î£ (n_k / n) * w_k
       where n_k = |D_k|, n = Î£ n_k
       
    4. Update global model
```

#### `strategy.py`

**Available Strategies:**

1. **FedAvg**: Standard weighted averaging
2. **FedProx**: Adds proximal term for heterogeneous data
3. **FedAdagrad**: Adaptive learning rate
4. **FedYogi**: Adam-like optimization
5. **SecureAggregation**: Differential privacy

### 4. Training Module (`training/`)

#### `train_centralized.py`

**Training Loop**
```
For each epoch:
    For each batch in train_loader:
        1. Forward pass
        2. Compute loss
        3. Backward pass
        4. Update weights
        
    Validate on validation set
    Save checkpoint if best model
    Check early stopping
    
Final evaluation on test set
```

#### `evaluate.py`

**Evaluation Pipeline**
```
Load trained model
    â†“
For each batch in test_loader:
    - Forward pass
    - Collect predictions and probabilities
    â†“
Calculate metrics:
    - Accuracy, Precision, Recall, F1
    - AUC-ROC
    - Confusion Matrix
    â†“
Generate visualizations:
    - Confusion matrix heatmap
    - ROC curve
    - Prediction distribution
```

## ğŸ”„ Data Flow

### Centralized Training

```
Raw Data
    â†“
[Data Loader]
    â†“
Preprocessing & Augmentation
    â†“
Batches (32 images)
    â†“
[Model]
    â†“
Predictions
    â†“
[Loss Function]
    â†“
Gradients
    â†“
[Optimizer]
    â†“
Updated Model
```

### Federated Training

```
Global Model (Server)
    â†“
Broadcast to Clients
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Client 1 â”‚Client 2 â”‚Client N â”‚
â”‚         â”‚         â”‚         â”‚
â”‚Local    â”‚Local    â”‚Local    â”‚
â”‚Training â”‚Training â”‚Training â”‚
â”‚         â”‚         â”‚         â”‚
â”‚Updates  â”‚Updates  â”‚Updates  â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
     â”‚         â”‚         â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
    Aggregate Updates (FedAvg)
              â†“
    Updated Global Model
              â†“
         Next Round
```

## ğŸ§  Model Architecture Details

### EfficientNet-B0 Backbone

```
Parameters: ~5.3M
Input: (B, 3, 224, 224)

Stem: Conv3x3 + BN + Swish
    â†“
MBConv Blocks (7 stages):
    - Mobile Inverted Bottleneck
    - Squeeze-and-Excitation
    - Skip connections
    â†“
Head: Conv1x1 + BN + Swish + GlobalAvgPool
    â†“
Output: (B, 1280)
```

### Custom Classification Head

```
Input: (B, 1280)
    â†“
Linear(1280 â†’ 512)
    â†“
ReLU
    â†“
Dropout(0.5)
    â†“
Linear(512 â†’ 256)
    â†“
ReLU
    â†“
Dropout(0.5)
    â†“
Linear(256 â†’ 2)
    â†“
Output: (B, 2) [logits for Real/Fake]
```

## ğŸ“Š Metrics and Evaluation

### Classification Metrics

**Confusion Matrix**
```
                Predicted
              Real    Fake
Actual Real    TN      FP
       Fake    FN      TP
```

**Computed Metrics**
- Accuracy = (TP + TN) / (TP + TN + FP + FN)
- Precision = TP / (TP + FP)
- Recall = TP / (TP + FN)
- F1-Score = 2 * (Precision * Recall) / (Precision + Recall)
- Specificity = TN / (TN + FP)

**ROC-AUC**
- Plot True Positive Rate vs False Positive Rate
- Area under curve (higher is better)
- Threshold-independent metric

## ğŸ” Privacy in Federated Learning

### Data Privacy
```
âœ“ Raw data never leaves client devices
âœ“ Only model parameters are shared
âœ“ Server cannot reconstruct individual data
âœ— Model updates may leak some information
```

### Differential Privacy (Optional)
```
For each parameter update:
    1. Clip gradient to bound sensitivity
    2. Add Gaussian noise: N(0, ÏƒÂ²)
    3. Noise scale proportional to privacy budget Îµ
    
Privacy-Accuracy Trade-off:
    More noise â†’ Better privacy, Lower accuracy
    Less noise â†’ Worse privacy, Higher accuracy
```

## ğŸ¯ Training Strategies

### Learning Rate Scheduling

**ReduceLROnPlateau**
```
If validation loss doesn't improve for N epochs:
    lr = lr * factor (e.g., 0.5)
```

**CosineAnnealing**
```
lr(t) = lr_min + 0.5 * (lr_max - lr_min) * (1 + cos(Ï€t/T))
```

### Early Stopping
```
Track best validation metric
If no improvement for patience epochs:
    Stop training
    Restore best model
```

## ğŸ”§ Optimization

### Optimizers

**Adam**
- Adaptive learning rate
- Momentum + RMSprop
- Good default choice

**SGD with Momentum**
- Classic optimizer
- May need careful tuning
- Can achieve better generalization

**AdamW**
- Adam with decoupled weight decay
- Better regularization
- Recommended for transformers

## ğŸ“ˆ Performance Considerations

### Memory Optimization
- Gradient accumulation for large batches
- Mixed precision training (FP16)
- Gradient checkpointing

### Speed Optimization
- Data loading: Multiple workers
- Pin memory for GPU transfer
- Prefetching batches

### Distributed Training
- Data parallelism across GPUs
- Model parallelism for large models
- Federated learning for privacy

## ğŸ”¬ Research Extensions

### Possible Improvements

1. **Model Architecture**
   - Vision Transformers (ViT)
   - EfficientNet-B4/B7 (larger models)
   - Multi-modal fusion (audio + video)

2. **Federated Learning**
   - Personalized federated learning
   - Federated transfer learning
   - Byzantine-robust aggregation

3. **Privacy**
   - Secure multi-party computation
   - Homomorphic encryption
   - Trusted execution environments

4. **Data**
   - Temporal modeling for videos
   - Face alignment preprocessing
   - Synthetic data generation

## ğŸ“š References

- **EfficientNet**: Tan & Le, 2019
- **Federated Learning**: McMahan et al., 2017
- **FedProx**: Li et al., 2020
- **Deepfake Detection**: Rossler et al., 2019 (FaceForensics++)

---

This architecture is designed to be:
- âœ… **Modular**: Easy to swap components
- âœ… **Scalable**: Works with any number of clients
- âœ… **Extensible**: Add new models and strategies
- âœ… **Privacy-preserving**: Federated learning by design
